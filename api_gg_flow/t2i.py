import requests
import json
import os
import time
import threading
from typing import Optional, Dict, Any, List, Union

# Recaptcha logic removed completely

# ============================================================================
# MODEL SUCCESS TRACKING SYSTEM
# ============================================================================
# Theo d√µi v√† ghi nh·ªõ model n√†o ƒëang ho·∫°t ƒë·ªông t·ªët
# T·∫•t c·∫£ workers s·∫Ω shared state n√†y ƒë·ªÉ h·ªçc t·ª´ nhau

_model_success_lock = threading.Lock()
_model_success_tracker = {
    "last_successful_model": "GEM_PIX_2",  # Model th√†nh c√¥ng g·∫ßn nh·∫•t
    "consecutive_failures": 0,              # S·ªë l·∫ßn fail li√™n ti·∫øp c·ªßa model hi·ªán t·∫°i
    "switch_threshold": 3,                  # S·ªë l·∫ßn fail tr∆∞·ªõc khi switch model
    "model_stats": {                        # Th·ªëng k√™ cho m·ªói model
        "GEM_PIX_2": {"success": 0, "failure": 0},
        "GEM_PIX": {"success": 0, "failure": 0},
    }
}


def _get_preferred_model() -> str:
    """
    L·∫•y model ƒë∆∞·ª£c ∆∞u ti√™n (model th√†nh c√¥ng g·∫ßn nh·∫•t)
    Thread-safe.
    """
    with _model_success_lock:
        return _model_success_tracker["last_successful_model"]


def _record_model_success(model_name: str):
    """
    Ghi nh·∫≠n model th√†nh c√¥ng
    - C·∫≠p nh·∫≠t last_successful_model
    - Reset consecutive_failures
    - TƒÉng success count
    Thread-safe.
    """
    with _model_success_lock:
        _model_success_tracker["last_successful_model"] = model_name
        _model_success_tracker["consecutive_failures"] = 0
        if model_name in _model_success_tracker["model_stats"]:
            _model_success_tracker["model_stats"][model_name]["success"] += 1


def _record_model_failure(model_name: str) -> bool:
    """
    Ghi nh·∫≠n model th·∫•t b·∫°i
    - TƒÉng consecutive_failures
    - TƒÉng failure count
    - Tr·∫£ v·ªÅ True n·∫øu c·∫ßn switch model
    Thread-safe.
    """
    with _model_success_lock:
        _model_success_tracker["consecutive_failures"] += 1
        if model_name in _model_success_tracker["model_stats"]:
            _model_success_tracker["model_stats"][model_name]["failure"] += 1
        
        # Ki·ªÉm tra c√≥ c·∫ßn switch model kh√¥ng
        should_switch = _model_success_tracker["consecutive_failures"] >= _model_success_tracker["switch_threshold"]
        return should_switch


def _switch_to_alternative_model(current_model: str) -> str:
    """
    Chuy·ªÉn sang model alternative
    GEM_PIX_2 <-> GEM_PIX
    Thread-safe.
    """
    with _model_success_lock:
        if current_model == "GEM_PIX_2":
            new_model = "GEM_PIX"
        else:
            new_model = "GEM_PIX_2"
        
        # C·∫≠p nh·∫≠t last_successful_model v√† reset consecutive_failures
        _model_success_tracker["last_successful_model"] = new_model
        _model_success_tracker["consecutive_failures"] = 0
        
        return new_model


def get_model_stats() -> Dict[str, Any]:
    """
    L·∫•y th·ªëng k√™ model (d√πng cho debug/monitoring)
    Thread-safe.
    """
    with _model_success_lock:
        return {
            "last_successful_model": _model_success_tracker["last_successful_model"],
            "consecutive_failures": _model_success_tracker["consecutive_failures"],
            "stats": _model_success_tracker["model_stats"].copy()
        }


def reset_model_stats():
    """
    Reset to√†n b·ªô model stats v·ªÅ m·∫∑c ƒë·ªãnh
    Thread-safe.
    """
    with _model_success_lock:
        _model_success_tracker["last_successful_model"] = "GEM_PIX_2"
        _model_success_tracker["consecutive_failures"] = 0
        _model_success_tracker["model_stats"] = {
            "GEM_PIX_2": {"success": 0, "failure": 0},
            "GEM_PIX": {"success": 0, "failure": 0},
        }


# Lazy import ƒë·ªÉ tr√°nh circular import
def _get_access_token():
    """Lazy import get_access_token"""
    try:
        from .get_acess_token import get_access_token
    except ImportError:
        from get_acess_token import get_access_token
    return get_access_token




def _normalize_ratio(value: Optional[str]) -> str:
    if not value:
        return ""
    return value.strip().upper().replace(" ", "").replace("X", ":")


_IMAGE_RATIO_ALIASES = {
    "16:9": "IMAGE_ASPECT_RATIO_LANDSCAPE",
    "9:16": "IMAGE_ASPECT_RATIO_PORTRAIT",
    "IMAGE_ASPECT_RATIO_LANDSCAPE": "IMAGE_ASPECT_RATIO_LANDSCAPE",
    "IMAGE_ASPECT_RATIO_PORTRAIT": "IMAGE_ASPECT_RATIO_PORTRAIT",
}


def convert_image_aspect_ratio(aspect_ratio: Optional[str], default: str = "IMAGE_ASPECT_RATIO_LANDSCAPE") -> str:
    """
    ƒê·ªìng nh·∫•t aspect ratio (16:9 ho·∫∑c 9:16) sang IMAGE_ASPECT_RATIO_*.
    H·ªó tr·ª£ truy·ªÅn v√†o 16:9/9:16 ho·∫∑c h·∫±ng IMAGE_ASPECT_RATIO_*.
    """
    normalized_value = _normalize_ratio(aspect_ratio)
    if normalized_value:
        resolved = _IMAGE_RATIO_ALIASES.get(normalized_value)
        if resolved:
            return resolved

    normalized_default = _normalize_ratio(default) or "16:9"
    return _IMAGE_RATIO_ALIASES.get(normalized_default, "IMAGE_ASPECT_RATIO_LANDSCAPE")



def download_image(url: str, output_path: str, timeout: int = 60) -> bool:
    """
    T·∫£i ·∫£nh t·ª´ URL xu·ªëng file
    
    Args:
        url: URL c·ªßa ·∫£nh c·∫ßn t·∫£i
        output_path: ƒê∆∞·ªùng d·∫´n file ƒë·ªÉ l∆∞u ·∫£nh
        timeout: Timeout cho request (gi√¢y)
    
    Returns:
        True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
    """
    try:
        response = requests.get(url, stream=True, timeout=timeout)
        response.raise_for_status()
        
        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
        os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True)
        
        with open(output_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        return True
    except Exception as e:
        print(f"L·ªói khi t·∫£i ·∫£nh t·ª´ {url}: {e}")
        return False


def get_media_generation_ids_from_t2i_response(data: Dict[str, Any]) -> List[str]:
    """
    Tr√≠ch xu·∫•t mediaGenerationId t·ª´ response c·ªßa batchGenerateImages/Video

    Args:
        data: Response JSON t·ª´ API batchGenerateImages/Video

    Returns:
        Danh s√°ch mediaGenerationId
    """
    media_ids = []
    
    # Ki·ªÉm tra n·∫øu data kh√¥ng h·ª£p l·ªá
    if not data or not isinstance(data, dict):
        print("‚ö†Ô∏è Response data kh√¥ng h·ª£p l·ªá ho·∫∑c r·ªóng")
        return media_ids
    
    # X·ª≠ l√Ω c·∫•u tr√∫c media[] (c·∫•u tr√∫c ch√≠nh th·ª©c)
    if 'media' in data and isinstance(data['media'], list) and len(data['media']) > 0:
        for idx, media_obj in enumerate(data['media']):
            if not isinstance(media_obj, dict):
                continue
                
            # mediaGenerationId c√≥ th·ªÉ n·∫±m ·ªü nhi·ªÅu v·ªã tr√≠
            media_id = None

            # Th·ª≠ t√¨m ·ªü c·∫•p ƒë·ªô media_obj tr∆∞·ªõc
            if 'mediaGenerationId' in media_obj:
                media_id = media_obj['mediaGenerationId']
            # Th·ª≠ t√¨m trong video.generatedVideo (cho video generation)
            elif 'video' in media_obj and isinstance(media_obj['video'], dict):
                video_obj = media_obj['video']
                # Th·ª≠ t√¨m trong generatedVideo tr∆∞·ªõc (c·∫•u tr√∫c m·ªõi nh·∫•t)
                if 'generatedVideo' in video_obj and isinstance(video_obj['generatedVideo'], dict):
                    generated_video = video_obj['generatedVideo']
                    if 'mediaGenerationId' in generated_video:
                        media_id = generated_video['mediaGenerationId']
                # Fallback: th·ª≠ t√¨m tr·ª±c ti·∫øp trong video
                elif 'mediaGenerationId' in video_obj:
                    media_id = video_obj['mediaGenerationId']
            # Th·ª≠ t√¨m trong image.generatedImage (cho image generation - fallback)
            elif 'image' in media_obj and isinstance(media_obj['image'], dict):
                image_obj = media_obj['image']
                # Th·ª≠ t√¨m trong generatedImage tr∆∞·ªõc (c·∫•u tr√∫c m·ªõi nh·∫•t)
                if 'generatedImage' in image_obj and isinstance(image_obj['generatedImage'], dict):
                    generated_image = image_obj['generatedImage']
                    if 'mediaGenerationId' in generated_image:
                        media_id = generated_image['mediaGenerationId']
                # Fallback: th·ª≠ t√¨m tr·ª±c ti·∫øp trong image
                elif 'mediaGenerationId' in image_obj:
                    media_id = image_obj['mediaGenerationId']
            
            if media_id:
                media_ids.append(media_id)
            else:
                print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y mediaGenerationId trong media[{idx}]")
    
    # Fallback: X·ª≠ l√Ω c·∫•u tr√∫c responses[] (c·∫•u tr√∫c c≈©)
    elif 'responses' in data and isinstance(data['responses'], list) and len(data['responses']) > 0:
        for response_obj in data['responses']:
            if isinstance(response_obj, dict):
                # Try videos first (for video generation)
                if 'videos' in response_obj:
                    videos = response_obj['videos']
                    if isinstance(videos, list) and len(videos) > 0:
                        video_obj = videos[0]
                        if isinstance(video_obj, dict) and 'mediaGenerationId' in video_obj:
                            media_ids.append(video_obj['mediaGenerationId'])
                # Fallback to images (for image generation)
                elif 'images' in response_obj:
                    images = response_obj['images']
                    if isinstance(images, list) and len(images) > 0:
                        image_obj = images[0]
                        if isinstance(image_obj, dict) and 'mediaGenerationId' in image_obj:
                            media_ids.append(image_obj['mediaGenerationId'])
    
    # Fallback: X·ª≠ l√Ω c·∫•u tr√∫c workflows[] (c·∫•u tr√∫c m·ªõi t·ª´ T2I response)
    if not media_ids and 'workflows' in data and isinstance(data['workflows'], list) and len(data['workflows']) > 0:
        print(f"üîç ƒêang ki·ªÉm tra workflows, c√≥ {len(data['workflows'])} workflow(s)")
        for workflow in data['workflows']:
            print(f"üîç Workflow: {workflow}")
            if isinstance(workflow, dict):
                # Th·ª≠ t√¨m primaryMediaId trong metadata
                metadata = workflow.get('metadata')
                if isinstance(metadata, dict) and 'primaryMediaId' in metadata:
                    media_ids.append(metadata['primaryMediaId'])
                    print(f"‚úì T√¨m th·∫•y mediaGenerationId t·ª´ workflows: {metadata['primaryMediaId']}")
                else:
                    print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y primaryMediaId trong metadata: {metadata}")

    # Fallback: X·ª≠ l√Ω c·∫•u tr√∫c operations[] (c·∫•u tr√∫c t·ª´ V2V response)
    if not media_ids and 'operations' in data and isinstance(data['operations'], list) and len(data['operations']) > 0:
        for operation in data['operations']:
            if isinstance(operation, dict):
                # Th·ª≠ t√¨m mediaGenerationId trong operation
                op_data = operation.get('operation')
                if isinstance(op_data, dict):
                    # C√≥ th·ªÉ mediaGenerationId n·∫±m ·ªü ƒë√¢y
                    if 'mediaGenerationId' in op_data:
                        media_ids.append(op_data['mediaGenerationId'])
                        print(f"‚úì T√¨m th·∫•y mediaGenerationId t·ª´ operations: {op_data['mediaGenerationId']}")

    # N·∫øu kh√¥ng t√¨m th·∫•y media_ids, in th√¥ng tin debug
    if not media_ids:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y mediaGenerationId trong response")
        print(f"   Response keys: {list(data.keys()) if isinstance(data, dict) else 'N/A'}")
        if isinstance(data, dict) and 'media' in data:
            print(f"   Media array length: {len(data['media']) if isinstance(data['media'], list) else 'N/A'}")
        if isinstance(data, dict) and 'workflows' in data:
            print(f"   Workflows length: {len(data['workflows']) if isinstance(data['workflows'], list) else 'N/A'}")
        if isinstance(data, dict) and 'operations' in data:
            print(f"   Operations length: {len(data['operations']) if isinstance(data['operations'], list) else 'N/A'}")

    return media_ids


def get_media_names_from_t2i_response(data: Dict[str, Any]) -> List[str]:
    """
    Tr√≠ch xu·∫•t field 'name' t·ª´ response c·ªßa batchGenerateImages.
    Tr·∫£ v·ªÅ danh s√°ch gi·ªØ nguy√™n th·ª© t·ª± xu·∫•t hi·ªán trong response.
    """
    media_names: List[str] = []

    if not data or not isinstance(data, dict):
        return media_names

    # Th·ª≠ t√¨m trong media[] tr∆∞·ªõc
    media_entries = data.get('media')
    if isinstance(media_entries, list) and media_entries:
        for idx, media_obj in enumerate(media_entries):
            if not isinstance(media_obj, dict):
                continue
            name_value = media_obj.get('name')
            if name_value:
                media_names.append(name_value)
            else:
                print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y name trong media[{idx}]")

    # Fallback: Th·ª≠ t√¨m trong workflows[]
    if not media_names and 'workflows' in data and isinstance(data['workflows'], list):
        for workflow in data['workflows']:
            if isinstance(workflow, dict):
                # Th·ª≠ t√¨m name trong workflow
                name_value = workflow.get('name')
                if name_value:
                    media_names.append(name_value)
                    print(f"‚úì T√¨m th·∫•y name t·ª´ workflows: {name_value}")

    return media_names


def create_batch_text_to_image(
    project_id: str,
    request_list: List[Dict[str, Any]],
    access_token: Optional[str] = None,
    output_dir: Optional[str] = None,
    verbose: bool = True,
    filename_prefix: Optional[str] = None,
    return_media_ids: bool = False,
    timeout: int = 180,
    max_retries: int = 15,
    retry_delay: float = 2.0,
    enable_model_fallback: bool = True
) -> Union[List[str], Dict[str, Any]]:
    """
    T·∫°o nhi·ªÅu ·∫£nh t·ª´ text (batch text-to-image) tr√™n Google AI Sandbox
    
    Args:
        project_id: ID c·ªßa project
        request_list: Danh s√°ch c√°c request objects. M·ªói request c√≥ c·∫•u tr√∫c:
            {
                "clientContext": {"sessionId": "..."} ho·∫∑c None (t·ª± ƒë·ªông t·∫°o),
                "seed": int,
                "imageModelName": "GEM_PIX" ho·∫∑c "GEM_PIX_2",
                "imageAspectRatio": "IMAGE_ASPECT_RATIO_LANDSCAPE" ho·∫∑c "IMAGE_ASPECT_RATIO_PORTRAIT",
                "prompt": "text prompt",
                "imageInputs": [
                    {
                        "name": "image_name_string",
                        "imageInputType": "IMAGE_INPUT_TYPE_REFERENCE"
                    },
                    ...
                ] (optional),
                "requestData": {
                    "promptInputs": [
                        {
                            "textInput": "text prompt"
                        }
                    ],
                    "imageGenerationRequestData": {
                        "imageGenerationImageInputs": [
                            {
                                "mediaGenerationId": "media_id_string",
                                "imageInputType": "IMAGE_INPUT_TYPE_REFERENCE"
                            },
                            ...
                        ]
                    }
                } (optional)
            }
        access_token: Access token (n·∫øu None s·∫Ω t·ª± ƒë·ªông l·∫•y)
        output_dir: Th∆∞ m·ª•c ƒë·ªÉ l∆∞u ·∫£nh (None = l∆∞u v√†o th∆∞ m·ª•c hi·ªán t·∫°i)
        verbose: In th√¥ng tin progress (m·∫∑c ƒë·ªãnh: True)
    
        return_media_ids: N·∫øu True, tr·∫£ v·ªÅ dict ch·ª©a output_paths, media_ids v√† media_names. 
                          N·∫øu False, ch·ªâ tr·∫£ v·ªÅ List[str] c√°c ƒë∆∞·ªùng d·∫´n file (m·∫∑c ƒë·ªãnh)
        timeout: Timeout cho request (gi√¢y), m·∫∑c ƒë·ªãnh 180 gi√¢y
        max_retries: S·ªë l·∫ßn th·ª≠ l·∫°i t·ªëi ƒëa khi g·∫∑p l·ªói 503/429 (m·∫∑c ƒë·ªãnh: 3)
        retry_delay: Th·ªùi gian ch·ªù gi·ªØa c√°c l·∫ßn th·ª≠ l·∫°i (gi√¢y, m·∫∑c ƒë·ªãnh: 2.0, s·ª≠ d·ª•ng exponential backoff)
        enable_model_fallback: N·∫øu True, s·∫Ω t·ª± ƒë·ªông th·ª≠ v·ªõi GEM_PIX_2 n·∫øu GEM_PIX fail (m·∫∑c ƒë·ªãnh: True)
    
    Returns:
        N·∫øu return_media_ids=False: Danh s√°ch ƒë∆∞·ªùng d·∫´n file ·∫£nh output (c√≥ th·ªÉ √≠t h∆°n s·ªë requests n·∫øu c√≥ l·ªói)
        N·∫øu return_media_ids=True: Dict v·ªõi keys: {"output_paths": List[str], "media_ids": List[str], "media_names": List[str]}
    
    V√≠ d·ª•:
        # V√≠ d·ª• v·ªõi imageInputs (c√°ch c≈©)
        request_list = [
            {
                "seed": 558802,
                "imageModelName": "GEM_PIX_2",
                "imageAspectRatio": "IMAGE_ASPECT_RATIO_LANDSCAPE",
                "prompt": "for the first photo use the second photo product. on the third photo background",
                "imageInputs": [
                    {
                        "name": "CAMaJDVjMTRmYTYxLTE5ODQtNDBmNC1iYzA0LWJjNGUwNjM5MmI4OCIDQ0FFKiQ1MjZmYzQzNC1kMmJlLTRkZGEtOThjNC1iYjY2MmU2ZDhlYjI",
                        "imageInputType": "IMAGE_INPUT_TYPE_REFERENCE"
                    },
                    {
                        "name": "CAMaJDU4YzdkMDRhLTBmZmMtNDMzNi1iMDc0LWVjNTVhYTliYjg3OCIDQ0FFKiQwN2MxYTlkYy1hYmU1LTQ5MzgtYTcxOC1iYTYxYzdkMjZhMzY",
                        "imageInputType": "IMAGE_INPUT_TYPE_REFERENCE"
                    }
                ]
            }
        ]
        
        # V√≠ d·ª• v·ªõi requestData (c√°ch m·ªõi)
        request_list = [
            {
                "seed": 558802,
                "imageModelName": "GEM_PIX_2",
                "imageAspectRatio": "IMAGE_ASPECT_RATIO_LANDSCAPE",
                "requestData": {
                    "promptInputs": [
                        {
                            "textInput": "cho nh√¢n v·∫≠t ·∫£nh th·ª© 1 s·ª≠ d·ª•ng s·∫£n ph·∫©m ·∫£nh th·ª© 2 trong b·ªëi c·∫£nh ch·ª£ t·∫øt"
                        }
                    ],
                    "imageGenerationRequestData": {
                        "imageGenerationImageInputs": [
                            {
                                "mediaGenerationId": "CAMaJDEzYzFjNmM4LTc4M2MtNDYwOC05MGRhLTk4NzRmODEwYmQxNyIDQ0FFKiQ2NzU2YTU3MC00ODUyLTRiY2EtYjMzZC03NGExNmZlYmYxNjk",
                                "imageInputType": "IMAGE_INPUT_TYPE_REFERENCE"
                            },
                            {
                                "mediaGenerationId": "CAMaJDNmMTJkNTQyLWVkYzktNDA0NC1hZDI0LTM1YzI3ZWM2ZDU1ZSIDQ0FFKiQ5OWQ0MzM4ZC0xZTIwLTRlNTAtYTQ0OC00NmZkNmExODQyMjc",
                                "imageInputType": "IMAGE_INPUT_TYPE_REFERENCE"
                            }
                        ]
                    }
                }
            }
        ]
        
        results = create_batch_text_to_image(
            project_id="6ca1d40d-9961-4f9c-a30d-d6cd1a9d1161",
            request_list=request_list,
            output_dir="./output_images"
        )
    """
    try:
        # L·∫•y access token n·∫øu ch∆∞a c√≥
        if not access_token:
            get_access_token_func = _get_access_token()
            access_token = get_access_token_func()
            if not access_token:
                print("Kh√¥ng th·ªÉ l·∫•y access token")
                return []
        
        # Chu·∫©n b·ªã requests
        processed_requests = []
        for req in request_list:
            # T·∫°o clientContext n·∫øu ch∆∞a c√≥
            if 'clientContext' not in req or not req['clientContext']:
                session_id = f";{int(time.time() * 1000)}"
                req['clientContext'] = {"sessionId": session_id}
            
            # Chu·∫©n h√≥a imageAspectRatio
            if 'imageAspectRatio' in req:
                req['imageAspectRatio'] = convert_image_aspect_ratio(req['imageAspectRatio'])
            else:
                req['imageAspectRatio'] = "IMAGE_ASPECT_RATIO_LANDSCAPE"

            # üéØ MODEL SELECTION: ∆Øu ti√™n d√πng model ƒë√£ th√†nh c√¥ng g·∫ßn nh·∫•t
            if 'imageModelName' not in req or not req['imageModelName']:
                # S·ª≠ d·ª•ng model th√†nh c√¥ng g·∫ßn nh·∫•t t·ª´ tracking system
                req['imageModelName'] = _get_preferred_model()

            # Chu·∫©n h√≥a imageInputs (n·∫øu c√≥)
            if 'imageInputs' in req and req['imageInputs']:
                # ƒê·∫£m b·∫£o m·ªói imageInput c√≥ imageInputType
                for img_input in req['imageInputs']:
                    if 'imageInputType' not in img_input:
                        img_input['imageInputType'] = "IMAGE_INPUT_TYPE_REFERENCE"
                    # Lo·∫°i b·ªè c√°c imageInput r·ªóng
                    if not img_input.get('name') or not img_input['name'].strip():
                        req['imageInputs'].remove(img_input)
                # N·∫øu sau khi l·ªçc kh√¥ng c√≤n imageInput n√†o, x√≥a field
                if not req['imageInputs']:
                    del req['imageInputs']
            
            processed_requests.append(req)

        if not processed_requests:
            if verbose:
                print("‚ö†Ô∏è Kh√¥ng c√≥ request h·ª£p l·ªá ƒë·ªÉ g·ª≠i")
            if return_media_ids:
                return {"output_paths": [], "media_ids": []}
            return []
        
        # Google sandbox throttles aggressively, keep each call ‚â§4 requests
        max_requests_per_call = 4
        request_chunks = [
            processed_requests[i:i + max_requests_per_call]
            for i in range(0, len(processed_requests), max_requests_per_call)
        ]
        total_chunks = len(request_chunks)
        
        # API endpoint
        url = f"https://aisandbox-pa.googleapis.com/v1/projects/{project_id}/flowMedia:batchGenerateImages"

        # Headers ƒë·∫ßy ƒë·ªß nh∆∞ request m·∫´u (s·∫Ω c·∫≠p nh·∫≠t recaptcha token sau)
        headers = {
            'accept': '*/*',
            'accept-encoding': 'gzip, deflate, br, zstd',
            'accept-language': 'en-US,en;q=0.9',
            'authorization': f'Bearer {access_token}',
            'content-length': '2131',
            'content-type': 'text/plain;charset=UTF-8',
            'origin': 'https://labs.google',
            'priority': 'u=1, i',
            'referer': 'https://labs.google/',
            'sec-ch-ua': '"Google Chrome";v="143", "Chromium";v="143", "Not A(Brand";v="24"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
            'sec-fetch-dest': 'empty',
            'sec-fetch-mode': 'cors',
            'sec-fetch-site': 'cross-site',
            'user-agent': 'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 10.0; Trident/5.0)',
            'x-browser-channel': 'stable',
            'x-browser-copyright': 'Copyright 2025 Google LLC. All Rights reserved.',
            'x-browser-validation': 'UujAs0GAwdnCJ9nvrswZ+O+oco0=',
            'x-browser-year': '2025',
            'x-client-data': 'CIe2yQEIorbJAQipncoBCOTsygEIlqHLAQiFoM0BCJGkzwE='
        }

        # T·∫°o th∆∞ m·ª•c output n·∫øu c·∫ßn
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)

        # üéØ Log preferred model (model th√†nh c√¥ng g·∫ßn nh·∫•t)
        if verbose:
            preferred_model = _get_preferred_model()
            stats = get_model_stats()
            print(f"\n{'='*60}")
            print(f"üéØ MODEL SELECTION")
            print(f"   Preferred model: {preferred_model}")
            print(f"   Model stats: {json.dumps(stats['stats'], indent=4)}")
            print(f"{'='*60}\n")

        # T·∫°o session_id chung cho t·∫•t c·∫£ requests
        shared_session_id = f";{int(time.time() * 1000)}"
        
        def _execute_single_chunk(chunk_requests, chunk_idx, total_chunks):
            # üîÑ MODEL SUCCESS TRACKING & SMART FALLBACK
            # - Attempt 1-3: Th·ª≠ v·ªõi preferred model (model th√†nh c√¥ng g·∫ßn nh·∫•t)
            # - Attempt 4+: N·∫øu enable_model_fallback=True, ghi nh·∫≠n failure v√† switch sang model kh√°c
            fallback_threshold = 3  # Gi·∫£m t·ª´ 5 xu·ªëng 3 ƒë·ªÉ switch nhanh h∆°n

            response = None
            data = None
            attempt = 1
            effective_max_retries = max_retries
            
            # üîÑ MODEL SUCCESS TRACKING & FALLBACK STRATEGY
            # L∆∞u model g·ªëc, n·∫øu l·ªói s·∫Ω fallback t·ª´ GEM_PIX sang GEM_PIX_2
            original_models = {}  # {index: original_model_name}
            for i, req in enumerate(chunk_requests):
                original_models[i] = req.get('imageModelName', _get_preferred_model())

            # L·∫•y model hi·ªán t·∫°i ƒëang d√πng (gi·∫£ s·ª≠ t·∫•t c·∫£ requests d√πng c√πng model)
            current_model = chunk_requests[0].get('imageModelName', _get_preferred_model())

            # S·ªë l·∫ßn th·ª≠ tr∆∞·ªõc khi fallback sang model kh√°c
            fallback_threshold = 3  # Gi·∫£m t·ª´ 5 xu·ªëng 3 ƒë·ªÉ switch nhanh h∆°n

            # Retry kh√¥ng gi·ªõi h·∫°n cho ƒë·∫øn khi th√†nh c√¥ng
            while True:
                # üîÑ M·∫∂C ƒê·ªäNH: Lu√¥n l·∫•y recaptcha token m·ªõi cho m·ªói attempt (token ch·ªâ d√πng 1 l·∫ßn)
                print(f"üîÑ [Chunk {chunk_idx}/{total_chunks} Attempt {attempt}] L·∫•y recaptcha token m·ªõi...")
                try:
                    try:
                        from .creat_token_recaptch import get_token
                    except ImportError:
                        from creat_token_recaptch import get_token
                    chunk_recaptcha_token = get_token()
                    if chunk_recaptcha_token:
                        print(f"   ‚úì Token m·ªõi: {chunk_recaptcha_token[:20]}...")
                    else:
                        print(f"   ‚ùå Kh√¥ng th·ªÉ l·∫•y token m·ªõi")
                        time.sleep(1)  # ƒê·ª£i 1 gi√¢y tr∆∞·ªõc khi th·ª≠ l·∫°i
                        attempt += 1
                        continue
                except Exception as token_error:
                    print(f"   ‚ùå L·ªói khi l·∫•y token m·ªõi: {token_error}")
                    time.sleep(2)  # ƒê·ª£i 2 gi√¢y tr∆∞·ªõc khi th·ª≠ l·∫°i
                    attempt += 1
                    continue

                # T·∫°o session_id m·ªõi cho m·ªói chunk
                chunk_session_id = f";{int(time.time() * 1000)}"

                # T·∫°o clientContext cho top level
                top_client_context = {
                    "recaptchaToken": chunk_recaptcha_token,
                    "projectId": project_id,
                    "tool": "PINHOLE",
                    "userPaygateTier": "PAYGATE_TIER_TWO"
                }

                # ƒê·∫£m b·∫£o m·ªói request c√≥ clientContext
                for req in chunk_requests:
                    if 'clientContext' not in req or not req['clientContext']:
                        req['clientContext'] = {}

                    # Th√™m c√°c field v√†o clientContext c·ªßa request
                    req['clientContext']['recaptchaToken'] = chunk_recaptcha_token
                    req['clientContext']['sessionId'] = chunk_session_id
                    if 'projectId' not in req['clientContext']:
                        req['clientContext']['projectId'] = project_id
                    if 'tool' not in req['clientContext']:
                        req['clientContext']['tool'] = "PINHOLE"

                body = {
                    "clientContext": top_client_context,
                    "requests": chunk_requests
                }

                # Re-encode body v·ªõi token m·ªõi
                body_text = json.dumps(body, ensure_ascii=False)
                body_bytes = body_text.encode('utf-8')

                if verbose:
                    print(f"\n[REQUEST] Chunk {chunk_idx}/{total_chunks}: Sending {len(chunk_requests)} request(s)")
                    print(f"[REQUEST] Body:")
                    print(json.dumps(body, indent=2, ensure_ascii=False))
                    print()

                # üîÑ MODEL SUCCESS TRACKING & SMART FALLBACK
                # - Attempt 1-3: Th·ª≠ v·ªõi preferred model (model th√†nh c√¥ng g·∫ßn nh·∫•t)
                # - Attempt 4+: N·∫øu enable_model_fallback=True, ghi nh·∫≠n failure v√† switch sang model kh√°c
                if enable_model_fallback and attempt > fallback_threshold:
                    # Ghi nh·∫≠n model hi·ªán t·∫°i ƒë√£ th·∫•t b·∫°i nhi·ªÅu l·∫ßn
                    should_switch = _record_model_failure(current_model)

                    if should_switch:
                        # Switch sang model alternative
                        new_model = _switch_to_alternative_model(current_model)

                        # C·∫≠p nh·∫≠t t·∫•t c·∫£ requests sang model m·ªõi
                        for i, req in enumerate(chunk_requests):
                            req['imageModelName'] = new_model

                        # Update body with fallback model
                        body["requests"] = chunk_requests

                        # Log th√¥ng tin model switch (ch·ªâ log 1 l·∫ßn khi v·ª´a chuy·ªÉn)
                        if verbose and attempt == fallback_threshold + 1:
                            print(f"\n{'='*60}")
                            print(f"üîÑ SMART MODEL SWITCH")
                            print(f"   Model {current_model} th·∫•t b·∫°i {fallback_threshold} l·∫ßn li√™n ti·∫øp")
                            print(f"   ‚ûú Chuy·ªÉn sang model {new_model}")
                            print(f"{'='*60}\n")

                        # C·∫≠p nh·∫≠t current_model
                        current_model = new_model

                # Log model ƒëang d√πng cho m·ªói attempt
                if verbose:
                    current_models = set(req.get('imageModelName', 'GEM_PIX_2') for req in chunk_requests)
                    if len(current_models) == 1:
                        model_name = list(current_models)[0]
                        if attempt == 1:
                            print(f"üéØ Attempt {attempt}: S·ª≠ d·ª•ng model {model_name}")
                        else:
                            print(f"üîÑ Attempt {attempt}: Retry v·ªõi model {model_name}")

                try:
                    body_text = json.dumps(body, ensure_ascii=False)
                    body_bytes = body_text.encode('utf-8')  # Encode UTF-8 ƒë·ªÉ h·ªó tr·ª£ ti·∫øng Vi·ªát
                    response = requests.post(
                        url,
                        headers=headers,
                        data=body_bytes,
                        timeout=timeout
                    )

                    response.raise_for_status()
                    data = response.json()

                    # ‚úÖ SUCCESS: Ghi nh·∫≠n model th√†nh c√¥ng
                    _record_model_success(current_model)

                    if verbose:
                        stats = get_model_stats()
                        print(f"\n‚úÖ API call th√†nh c√¥ng v·ªõi model {current_model}")
                        print(f"   üìä Model stats: {stats['stats'][current_model]}")

                    break
                
                except requests.exceptions.HTTPError as e:
                    status_code = e.response.status_code if hasattr(e, 'response') and e.response else None
                    
                    # Original retry logic for 503/429
                    if status_code in [503, 429]:
                        if status_code == 429:
                            if effective_max_retries < 5:
                                effective_max_retries = 5
                            
                            base_delay = 30.0
                            delay = min(base_delay * (2 ** (attempt - 1)), 120.0)
                            
                            # Retry kh√¥ng gi·ªõi h·∫°n
                            if verbose:
                                print(f"‚ö†Ô∏è L·ªói 429 (Too Many Requests - Rate Limited)")
                                print(f"   Chunk {chunk_idx}/{total_chunks}: ƒê·ª£i {delay:.1f} gi√¢y tr∆∞·ªõc khi th·ª≠ l·∫°i...")
                                print(f"   Th·ª≠ l·∫°i l·∫ßn {attempt + 1}...")
                            time.sleep(delay)
                            attempt += 1
                            continue
                        else:
                            # L·ªói 503 - Retry kh√¥ng gi·ªõi h·∫°n
                            delay = retry_delay * (2 ** (attempt - 1))
                            if verbose:
                                print(f"‚ö†Ô∏è L·ªói 503 (Service Unavailable)")
                                print(f"   Chunk {chunk_idx}/{total_chunks}: Th·ª≠ l·∫°i sau {delay:.1f} gi√¢y...")
                                print(f"   Th·ª≠ l·∫°i l·∫ßn {attempt + 1}...")
                            time.sleep(delay)
                            attempt += 1
                            continue
                    
                    # C√°c l·ªói HTTP kh√°c (500, etc.) - Retry kh√¥ng gi·ªõi h·∫°n
                    if status_code in [500]:
                        delay = retry_delay * (2 ** (attempt - 1))
                        if verbose:
                            print(f"‚ö†Ô∏è L·ªói 500 (Internal Server Error)")
                            print(f"   Chunk {chunk_idx}/{total_chunks}: Th·ª≠ l·∫°i sau {delay:.1f} gi√¢y...")
                            print(f"   Th·ª≠ l·∫°i l·∫ßn {attempt + 1}...")
                        time.sleep(delay)
                        attempt += 1
                        continue
                    
                    raise
                
                except requests.exceptions.RequestException as e:
                    # Retry kh√¥ng gi·ªõi h·∫°n cho l·ªói k·∫øt n·ªëi
                    delay = retry_delay * (2 ** (attempt - 1))
                    if verbose:
                        print(f"‚ö†Ô∏è L·ªói k·∫øt n·ªëi: {e}")
                        print(f"   Chunk {chunk_idx}/{total_chunks}: Th·ª≠ l·∫°i sau {delay:.1f} gi√¢y...")
                        print(f"   Th·ª≠ l·∫°i l·∫ßn {attempt + 1}...")
                    time.sleep(delay)
                    attempt += 1
                    continue
            
            if response is None or data is None:
                raise requests.exceptions.RequestException(
                    f"Kh√¥ng th·ªÉ l·∫•y response t·ª´ API sau t·∫•t c·∫£ c√°c l·∫ßn th·ª≠ (chunk {chunk_idx}/{total_chunks})"
                )
            
            if verbose:
                print(f"\n[RESPONSE] Chunk {chunk_idx}/{total_chunks} Status: {response.status_code}")
                print(f"[RESPONSE] Body:")
                print(json.dumps(data, indent=2, ensure_ascii=False))
                print()
                print("‚úì API call th√†nh c√¥ng cho chunk hi·ªán t·∫°i")
                print()
            
            # Kh√¥ng c·∫ßn download ·∫£nh, ch·ªâ l·∫•y mediaGenerationId
            chunk_output_paths = []
            
            chunk_media_payload = {"ids": [], "names": []}
            if return_media_ids:
                chunk_media_ids = get_media_generation_ids_from_t2i_response(data)
                chunk_media_names = get_media_names_from_t2i_response(data)
                chunk_media_payload["ids"] = chunk_media_ids
                chunk_media_payload["names"] = chunk_media_names
                if verbose:
                    if chunk_media_ids:
                        print(f"üìã Chunk {chunk_idx}: Media Generation IDs: {chunk_media_ids}")
                    else:
                        print(f"‚ö†Ô∏è Chunk {chunk_idx}: Kh√¥ng t√¨m th·∫•y mediaGenerationId trong response")
                        print(f"   Response structure: {list(data.keys()) if isinstance(data, dict) else 'N/A'}")
                    if chunk_media_names:
                        print(f"üìã Chunk {chunk_idx}: Media names: {chunk_media_names}")

            # Th√¥ng b√°o th√†nh c√¥ng khi c√≥ mediaGenerationId
            if chunk_media_payload.get("ids"):
                print(f"‚úì Chunk {chunk_idx}: T·∫°o th√†nh c√¥ng {len(chunk_media_payload['ids'])} ·∫£nh")

            # Tr·∫£ v·ªÅ token ƒë·ªÉ d√πng cho header
            chunk_media_payload["token"] = chunk_recaptcha_token
            return chunk_output_paths, chunk_media_payload
        
        all_output_paths = []
        all_media_ids: List[str] = []
        all_media_names: List[str] = []
        
        for chunk_idx, chunk_requests in enumerate(request_chunks, 1):
            chunk_paths, chunk_media_payload = _execute_single_chunk(chunk_requests, chunk_idx, total_chunks)
            all_output_paths.extend(chunk_paths)
            if return_media_ids:
                chunk_media_ids = chunk_media_payload.get("ids", [])
                chunk_media_names = chunk_media_payload.get("names", [])
                if chunk_media_ids:
                    all_media_ids.extend(chunk_media_ids)
                if chunk_media_names:
                    all_media_names.extend(chunk_media_names)
        
        # Th√¥ng b√°o k·∫øt qu·∫£
        if verbose and (all_output_paths or all_media_ids):
            print()
            print("=" * 60)
            if all_output_paths:
                print(f"ƒê√£ t·∫£i th√†nh c√¥ng {len(all_output_paths)} ·∫£nh sau {total_chunks} chunk")
                for path in all_output_paths:
                    if os.path.exists(path):
                        file_size = os.path.getsize(path) / (1024 * 1024)
                        print(f"  - {path} ({file_size:.2f} MB)")
            if all_media_ids:
                print(f"ƒê√£ t·∫°o th√†nh c√¥ng {len(all_media_ids)} mediaGenerationId sau {total_chunks} chunk")
                print(f"Media IDs: {all_media_ids}")
            print()
        
        if return_media_ids:
            return {
                "output_paths": all_output_paths,
                "media_ids": all_media_ids,
                "media_names": all_media_names
            }
        
        return all_output_paths
        
    except requests.exceptions.RequestException as e:
        error_msg = f"L·ªói khi g·ªçi API: {e}"
        print(error_msg)
        
        if hasattr(e, 'response') and e.response is not None:
            status_code = e.response.status_code
            print(f"\n[RESPONSE] Status: {status_code}")
            print(f"[RESPONSE] Body:")
            try:
                error_data = e.response.json()
                print(json.dumps(error_data, indent=2, ensure_ascii=False))
                
                # Th√¥ng b√°o r√µ r√†ng h∆°n cho l·ªói 503
                if status_code == 503:
                    print("\n‚ö†Ô∏è L·ªñI 503 - SERVICE UNAVAILABLE")
                    print("   Google API server ƒëang qu√° t·∫£i ho·∫∑c ƒëang b·∫£o tr√¨.")
                    print("   ƒê√£ th·ª≠ l·∫°i nhi·ªÅu l·∫ßn nh∆∞ng kh√¥ng th√†nh c√¥ng.")
                    print("   Vui l√≤ng th·ª≠ l·∫°i sau v√†i ph√∫t.")
                elif status_code == 429:
                    print("\n‚ö†Ô∏è L·ªñI 429 - TOO MANY REQUESTS")
                    print("   ƒê√£ v∆∞·ª£t qu√° gi·ªõi h·∫°n s·ªë l∆∞·ª£ng request.")
                    print("   Vui l√≤ng ƒë·ª£i m·ªôt l√∫c tr∆∞·ªõc khi th·ª≠ l·∫°i.")
            except:
                print(e.response.text)
            print()
        else:
            print("\n‚ö†Ô∏è L·ªñI K·∫æT N·ªêI")
            print("   Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn Google API server.")
            print("   Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi internet v√† th·ª≠ l·∫°i.")
            print()
        return []
    except json.JSONDecodeError as e:
        print(f"L·ªói khi parse JSON response: {e}")
        return []
    except Exception as e:
        print(f"L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}")
        import traceback
        traceback.print_exc()
        return []


def create_text_to_image(
    project_id: str,
    prompt: str,
    seed: int,
    imageAspectRatio: str = "IMAGE_ASPECT_RATIO_LANDSCAPE",
    imageInputs_name: Optional[Union[str, List[str]]] = None,
    imageModelName: str = "GEM_PIX_2",
    access_token: Optional[str] = None,
    output_dir: Optional[str] = None,
    verbose: bool = True,
    filename_prefix: Optional[str] = None,
    enable_model_fallback: bool = True
) -> Optional[str]:
    """
    T·∫°o ·∫£nh t·ª´ text (text-to-image) tr√™n Google AI Sandbox
    
    Args:
        project_id: ID c·ªßa project
        prompt: Prompt m√¥ t·∫£ ·∫£nh c·∫ßn t·∫°o
        seed: Seed ƒë·ªÉ t·∫°o ·∫£nh (s·ªë nguy√™n)
        imageAspectRatio: T·ª∑ l·ªá khung h√¨nh. Nh·∫≠n "16:9"/"9:16" ho·∫∑c
            "IMAGE_ASPECT_RATIO_LANDSCAPE"/"IMAGE_ASPECT_RATIO_PORTRAIT"
            (t·ª± ƒë·ªông chuy·ªÉn v·ªÅ h·∫±ng IMAGE_ASPECT_RATIO_* t∆∞∆°ng ·ª©ng)
        imageInputs_name: T√™n c·ªßa image input (string) ho·∫∑c danh s√°ch t√™n (list).
            N·∫øu c√≥, s·∫Ω th√™m v√†o imageInputs. H·ªó tr·ª£ nhi·ªÅu imageInputs.
        imageModelName: T√™n model ("GEM_PIX" ho·∫∑c "GEM_PIX_2", m·∫∑c ƒë·ªãnh: "GEM_PIX_2")
        access_token: Access token (n·∫øu None s·∫Ω t·ª± ƒë·ªông l·∫•y)
        output_dir: Th∆∞ m·ª•c ƒë·ªÉ l∆∞u ·∫£nh (None = l∆∞u v√†o th∆∞ m·ª•c hi·ªán t·∫°i)
        verbose: In th√¥ng tin progress (m·∫∑c ƒë·ªãnh: True)
        enable_model_fallback: N·∫øu True, t·ª± ƒë·ªông fallback sang GEM_PIX_2 n·∫øu GEM_PIX fail (m·∫∑c ƒë·ªãnh: True)
    
    Returns:
        ƒê∆∞·ªùng d·∫´n file ·∫£nh output n·∫øu th√†nh c√¥ng, None n·∫øu th·∫•t b·∫°i
    
    V√≠ d·ª•:
        # M·ªôt imageInput
        result = create_text_to_image(
            project_id="6ca1d40d-9961-4f9c-a30d-d6cd1a9d1161",
            prompt="anh tu·∫•n ƒë·∫πp trai",
            seed=492402,
            imageAspectRatio="IMAGE_ASPECT_RATIO_LANDSCAPE",
            imageInputs_name="CAMaJDBiYWE4NTkwLTExYjQtNGZmMC05Mjk3LTkzODNlMjgxZDM1YiIDQ0FFKiQwMjllZDVjZC1jZjA1LTRiNzEtYTY1OC1jYjE1NDMxZWYyYTM"
        )
        
        # Nhi·ªÅu imageInputs
        result = create_text_to_image(
            project_id="6ca1d40d-9961-4f9c-a30d-d6cd1a9d1161",
            prompt="for the first photo use the second photo product. on the third photo background",
            seed=558802,
            imageModelName="GEM_PIX_2",
            imageInputs_name=[
                "CAMaJDVjMTRmYTYxLTE5ODQtNDBmNC1iYzA0LWJjNGUwNjM5MmI4OCIDQ0FFKiQ1MjZmYzQzNC1kMmJlLTRkZGEtOThjNC1iYjY2MmU2ZDhlYjI",
                "CAMaJDU4YzdkMDRhLTBmZmMtNDMzNi1iMDc0LWVjNTVhYTliYjg3OCIDQ0FFKiQwN2MxYTlkYy1hYmU1LTQ5MzgtYTcxOC1iYTYxYzdkMjZhMzY",
                "CAMaJGQ5NmRlMjlhLWFjZjEtNGE4OC05MTA2LTljNTUzMjc4MzI4ZiIDQ0FFKiRlYjdlZGRhOC0yNTBiLTQ5ZDQtYjk2OC04YjdkZDQ4MjY5MzI"
            ]
        )
    """
    try:
        # Chu·∫©n b·ªã imageInputs
        image_inputs = []
        if imageInputs_name:
            if isinstance(imageInputs_name, str):
                # M·ªôt imageInput
                if imageInputs_name.strip():
                    image_inputs = [
                        {
                            "name": imageInputs_name.strip(),
                            "imageInputType": "IMAGE_INPUT_TYPE_REFERENCE"
                        }
                    ]
            elif isinstance(imageInputs_name, list):
                # Nhi·ªÅu imageInputs
                for name in imageInputs_name:
                    if name and str(name).strip():
                        image_inputs.append({
                            "name": str(name).strip(),
                            "imageInputType": "IMAGE_INPUT_TYPE_REFERENCE"
                        })
        
        # T·∫°o request object
        request_obj = {
            "seed": seed,
            "imageModelName": imageModelName,
            "imageAspectRatio": convert_image_aspect_ratio(imageAspectRatio),
            "prompt": prompt
        }
        
        # Th√™m imageInputs n·∫øu c√≥
        if image_inputs:
            request_obj["imageInputs"] = image_inputs
        
        # S·ª≠ d·ª•ng h√†m batch ƒë·ªÉ x·ª≠ l√Ω
        results = create_batch_text_to_image(
            project_id=project_id,
            request_list=[request_obj],
            access_token=access_token,
            output_dir=output_dir,
            verbose=verbose,
            filename_prefix=filename_prefix,
            timeout=180,  # Timeout 180 gi√¢y cho text-to-image
            enable_model_fallback=enable_model_fallback
        )
        
        # Tr·∫£ v·ªÅ ·∫£nh ƒë·∫ßu ti√™n (t∆∞∆°ng th√≠ch v·ªõi API c≈©)
        return results[0] if results else None
        
    except Exception as e:
        print(f"L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}")
        import traceback
        traceback.print_exc()
        return None




if __name__ == "__main__":
    """
    Test v·ªõi payload c√≥ nhi·ªÅu requests v√† nhi·ªÅu imageInputs
    """
    project_id = "e3fefbbe-03db-432d-8174-1b837281b6b6"
    
    # Payload test v·ªõi 1 requests c√≥ imageInputs
    request_list = [
        {
            "seed": 918949,
            "imageModelName": "GEM_PIX_2",
            "imageAspectRatio": "IMAGE_ASPECT_RATIO_PORTRAIT",
            "prompt": "C·∫£nh cu·ªëi: n·ªØ -mai ƒë·ª©ng c·∫°nh m≈©i xe, l·ªách sang m·ªôt b√™n ƒë·ªÉ kh√¥ng che bi·ªÉn s·ªë 30L 389 98, NH√ÇN V·∫¨T B·∫ÆT BU·ªòC ƒê·ª®NG C·∫†NH MUI XE, KH√îNG CHE BI·ªÇN S·ªê XE, BI·ªÇN S·ªê XE PH·∫¢I R√ï R√ÄNG KH√îNG B·ªä CHE L·∫§P, ƒê·ª®NG Y√äN nh√¨n th·∫≥ng camera; G√ìC QUAY R·ªòNG hi·ªÉn th·ªã tr·ªçn nh√¢n v·∫≠t full body, tr·ªçn ƒë·∫ßu xe, tr·ªçn bi·ªÉn s·ªë v√† t∆∞·ªùng showroom ph√∫ gia. Kh√¥ng ch·ªØ overlay. ƒê√¢y l√† ph·∫ßn tho·∫°i ƒë·ªÉ ƒë·ªçc, KH√îNG ƒë∆∞·ª£c hi·ªÉn th·ªã tr√™n video: n·ªØ -mai: \"S·ªë 3 t√†i kh√≠ h·ªôi t·ª•, c√πng 89 ph√°t tri·ªÉn b·ªÅn b·ªâ, ƒë√¢y l√† b·∫°n ƒë·ªìng h√†nh tr√™n m·ªçi h√†nh tr√¨nh th·ªãnh v∆∞·ª£ng. H√£y ƒë·∫øn ph√∫ gia ƒë·ªÉ ƒë√≥n t√†i l·ªôc ngay h√¥m nay!\"",
            "imageInputs": [
                {
                    "name": "CAMaJDA2MGRkNTQwLWMwYjItNGZkOS04ZThiLTAyMTVhYjlmZjA3NCIDQ0FFKiQzMDhjZmVjNi05NWI1LTRhM2QtYTFjNC0yMzY3YWY1N2EzNzk",
                    "imageInputType": "IMAGE_INPUT_TYPE_REFERENCE"
                }
            ]
        }
    ]
    
    print("=" * 80)
    print("TEST BATCH TEXT-TO-IMAGE V·ªöI NHI·ªÄU REQUESTS V√Ä NHI·ªÄU IMAGE INPUTS")
    print("=" * 80)
    print(f"\nS·ªë l∆∞·ª£ng requests: {len(request_list)}")
    for i, req in enumerate(request_list, 1):
        print(f"\nRequest {i}:")
        print(f"  - Seed: {req['seed']}")
        print(f"  - Model: {req['imageModelName']}")
        print(f"  - Aspect Ratio: {req['imageAspectRatio']}")
        print(f"  - Prompt: {req['prompt'][:50]}...")
        print(f"  - S·ªë imageInputs: {len(req.get('imageInputs', []))}")
        if 'clientContext' in req:
            print(f"  - ClientContext: {req['clientContext']}")
        else:
            print(f"  - ClientContext: (s·∫Ω t·ª± ƒë·ªông t·∫°o)")
    print("\n" + "=" * 80)
    print()
    
    # G·ªçi API batch
    results = create_batch_text_to_image(
        project_id=project_id,
        request_list=request_list,
        output_dir="./output_images",
        verbose=True
    )
    
    # In k·∫øt qu·∫£
    print("\n" + "=" * 80)
    print("K·∫æT QU·∫¢")
    print("=" * 80)
    if results:
        print(f"\n‚úì ƒê√£ t·∫°o th√†nh c√¥ng {len(results)} ·∫£nh:")
        for i, path in enumerate(results, 1):
            print(f"  {i}. {path}")
    else:
        print("\n‚úó Kh√¥ng th·ªÉ t·∫°o ·∫£nh")
    print("=" * 80)


